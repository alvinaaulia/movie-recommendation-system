# -*- coding: utf-8 -*-
"""MovieLens-Recommendation-System

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GQG2lbCQwBROJg-NoFqPaXktzbBea_WJ

# Data Understanding

Proses ini dilakukan untuk memahami data yang terdapat pada dataset-dataset yang terdapat pada MovieLens 20M.
Dataset MovieLens 20M terdiri dari 6 file utama:
- ```movie.csv``` (27278 baris): metadata film
- ```rating.csv``` (20000263 baris): interaksi userâ€“film
- ```tag.csv``` (465564 baris): tag dari user
- ```link.csv```(27278 baris): berisi identifier link di sumber lain
- ```genome_scores.csv``` (11709768 baris): nilai relevansi tags
- ```genome_tags.csv``` (1128 baris): deskripsi tags

Fitur penting:
- ```movieId```: ID film unik
- ```title```: judul film
- ```genres```: genre film (dipisahkan oleh '|')
- ```userId```: ID user
- ```rating```: nilai rating user (0.5-5.0)
- ```tag```: anotasi pendek dari user
- ```timestamp```: waktu interaksi dalam format UNIX time (baik di rating maupun tag)

Fitur tambahan:
- `imdbId` : identifier (kode unik) sumber film di Internet Movie Database ID
- `tmdbId` : identifier (kode unik) sumber film di The Movie Database ID
- `relevance` : nilai relevansi tag terhadap film
- `tagId` : ID unik tag
"""

from google.colab import files
files.upload()

!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

!kaggle datasets download -d grouplens/movielens-20m-dataset
!unzip movielens-20m-dataset.zip -d movielens-dataset

import pandas as pd
import numpy as np
import seaborn as sns
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import tensorflow as tf
from tensorflow.keras import layers
import matplotlib.pyplot as plt

tags = pd.read_csv('movielens-dataset/tag.csv')
rating = pd.read_csv('movielens-dataset/rating.csv')
movie = pd.read_csv('movielens-dataset/movie.csv')
link = pd.read_csv('movielens-dataset/link.csv')
genome_scores = pd.read_csv('movielens-dataset/genome_scores.csv')
genome_tags = pd.read_csv('movielens-dataset/genome_tags.csv')

print('Jumlah baris tags: ', len(tags.userId))
print('Jumlah baris rating: ', len(rating.userId))
print('Jumlah baris movie: ', len(movie.movieId))
print('Jumlah baris link: ', len(link.movieId))
print('Jumlah baris genome_scores: ', len(genome_scores.movieId))
print('Jumlah baris genome_tags: ', len(genome_tags.tagId))

print('Jumlah data unik tags: ', len(tags.userId.unique()))
print('Jumlah data unik rating: ', len(rating.userId.unique()))
print('Jumlah data unik movie: ', len(movie.movieId.unique()))
print('Jumlah data unik link: ', len(link.movieId.unique()))
print('Jumlah data unik genome_scores: ', len(genome_scores.movieId.unique()))
print('Jumlah data unik genome_tags: ', len(genome_tags.tagId.unique()))

"""**Insight:**
- Berdasarkan hasil di atas terdapat 7801 data unik pada DataFrame tags dengan total baris 465564, 138493 data unik pada rating dengan total baris 20000263, 27278 data unik pada DataFrame movie dengan total baris 27278, 27278 data unik pada DataFrame link dengan total baris 27278, 10381 data unik pada DataFrame genome_scores dengan total baris 11709768, dan 1128 data unik pada DataFrame genome_tags dengan total baris 1128.

# Univariate Exploratory Data Analysis

- Pada tahap ini dilakukan pemahaman tentang data yang akan diolah, manakah dataframe yang cocok untuk digunakan pada Content-Based Filtering dan Collaborative Filtering.
- Relevansi terhadap kedua teknik filtering. Pada **Content-Based Filtering** hanya membutuhkan data deskripsi item (film) seperti genre dan tag, dan itu sudah cukup dengan movie.csv + tag.csv:
  * **movie.csv**: ```movieId```, ```title```, ```genres```.
  * **tag.csv**: ```userId```, ```movieId```, ```tag```, ```timestamp```.

  Dan untuk **Collaborative Filtering** hanya membutuhkan data user, item, rating dan itu sudah cukup dengan menggunakan rating.csv yang berisi features ```userId```, ```movieId```, ```rating```, ```timestamp```.
- Tiga dataset (```movie.csv```, ```tag.csv```, dan ```rating.csv```) sudah cukup dan optimal untuk membangun sistem rekomendasi dengan dua pendekatan utama (content-based dan collaborative). Dataset lainnya opsional untuk peningkatan performa dan fitur lanjutan.

## Movie
"""

movie.info()

"""- Berdasarkan data di atas, dataframe movie memiliki 27278 data yang terdiri dari 3 features yaitu movieId, title, dan genres."""

print('Banyak data movie: ', len(movie.movieId.unique()))
print('Banyak data judul unik: ', len(movie.title.unique()))
print('Banyak data genres: ', len(movie.genres.unique()))

"""- Berdasarkan hasil di atas, pada dataframe movie terdapat 27278 data movie dengan 272762 judul unik dan 1342 genres."""

print('Distribusi: ', movie['genres'].value_counts())

"""- Hasil distribusi di atas menunjukkan bahwa Drama adalah genre dengan total movie paling banyak yaitu 4520.

## Tags
"""

tags.info()

"""- Berdasarkan data di atas, terdapat 465564 total data dengan 4 features yaitu userId, movieId, tag, dan timestamp."""

print('Banyak data tags:', len(tags.tag.unique()))
print('Banyak data user pemberi tags: ', len(tags.userId.unique()))
print('Banyak data movie yang diberi tags: ', len(tags.movieId.unique()))

"""- Berdasarkan hasil di atas terdapat 38644 data unik pada tags, 7801 data user yang memberikan tags, dan 19545 data movie yang diberi tags."""

print('Distribusi: ', tags['tag'].value_counts())

"""- Berdasarkan hasil distribusi di atas diketahui bahwa sci-fi adalah tag dengan movie terbanyak yaitu sejumlah 3384, yang disusul oleh based on a book dengan total movie 3281, dan lain-lain.

## Rating
"""

rating.info()

"""- Berdasarkan hasil di atas diketahui bahwa total data pada dataframe rating adalah 20000263 dengan 4 features yaitu userId, movieId, rating, dan timestamp."""

print('Banyak data user pemberi rating: ', len(rating.userId.unique()))
print('Banyak data movie yang diberi rating: ', len(rating.movieId.unique()))

"""- Berdasarkan hasil di atas ketahui bahwa terdapat 138493 user yang telah memberi rating pada movie dan terdapat 26744 data movie yang telah diberikan rating."""

print('Distribusi: ', rating['rating'].value_counts().head(10))

"""- Berdasarkan hasil distribusi di atas dapat diketahui bahwa sebagian besar user memberikan rating sebesar 4.0/5 dengan total yang telah memberikan rating 4.0 adalah 5561926. Di susul dengan rating 3.0/5 dengan banyak user 4291193, dan rating tertinggi yaitu 5.0 diberikan oleh sebanyak 2898660 user.

## Link
"""

link.info()

"""- Berdasarkan hasil di atas terdapat 27278 total data yang terdapat pada dataframe link dengan 3 features yang terdiri dari movieId, imdbId, dan terakhir dengan jumlah data yang berbeda yaitu sebanyak 27026 adalah tmdbId."""

print('Banyak data ID unik movie di IMD: ', len(link.imdbId.unique()))
print('Banyak data ID unik movie di TMDB: ', len(link.tmdbId.unique()))

"""- Berdasarkan hasil di atas terdapat total 27278 data unik yang terdapat pada feature imdbId dan 27009 data unik pada feature tmdbId.

## Genome Scores
"""

genome_scores.info()

"""- Berdasarkan data di atas terdapat total sejumlah 11709768 data entries pada dataframe genome_scores yang terdiri dari 3 features yaitu movieId, tagId, dan feature relevance."""

print('Distribusi 10 nilai relevansi paling banyak: ', genome_scores['relevance'].value_counts().head(10))

"""- Berdasarkan hasil distribusi di atas nilai relevansi yang paling banyak dimiliki oleh movie adalah sebesar 0.01400 yang mana ini berarti sebanyak 37940 tags memiliki nilai relevansi yang cukup baik daripada yang lainnya (biasanya semakin menyentuh angka 1 maka relevansi semakin baik). Dari hasil tersebut terlihat bahwa nilai relevansi yang paling tinggi adalah sebesar 0.0450 dengan nilai 0.01125 sebagai nilai relevansi yang paling kecil.

## Genome Tags
"""

genome_tags.info()

"""- Berdasarkan hasil di atas diketahui bahwa terdapat 1128 total baris yang ada pada dataframe genome_tags dengan 2 features yaitu tagId dan tag."""

genome_tags['tag'].head(10)

"""# Data Preparation

Tahap ini dilakukan untuk menyiapkan data seperti penanganan missing value, duplikasi, penggabungan feature yang diperlukan, hingga split data train dan validation sebelum digunakan dalam Content-Based Filtering dan Collaborative Filtering agar hasil rekomendasi yang diberikan dapat efektif.

## Mengatasi Missing Value

Proses ini dilakukan untuk menghilangkan nilai yang hilang yang akan berpengaruh buruk pada hasil rekomendasi pada CBF maupun Collaborative Filtering.

### tags
"""

tags.isna().sum()

"""**Insight:**
- Terdapat sebanyak 16 data yang bernilai NaN atau null, dan karena data null ini terbilang masih sangat sedikit dibandingkan dengan total data keseluruhan maka akan lebih baik jika nilai NaN itu dihapus.
"""

tags.dropna(subset=['tag'], inplace=True)

tags.isna().sum()

"""- Kini sudah tidak ada missing value.

### rating
"""

rating.isna().sum()

"""**Insight:** Tidak ada missing values.

## Mengatasi Duplikasi

Tahap ini dilakukan untuk mendeteksi dan mengatasi duplikasi yang nanti akan berpengaruh pada hasil rekomendasi.
"""

tags.duplicated().sum()

tags.shape

rating.shape

"""**Insight:**
- DataFrame movies_tags berisi 19545 data dengan 5 features dan DataFrame rating memiliki 20.000.263 data dengan 4 features.

## Agregasi data berdasarkan movieId
"""

movie_tags = tags.groupby('movieId')['tag'].apply(lambda x: ' '.join(x)).reset_index()

"""- Melakukan pengelompokan (grouping) dan penggabungan (aggregation) data berdasarkan movieId.

## Penggabungan Tag per Film

Proses ini digunakan untuk mempersiapkan data teks (genre dan tag) dari film agar siap diolah dalam model Content-Based Filtering.
"""

movies_tags = pd.merge(movie, movie_tags, on='movieId', how='left')
movies_tags['combined'] = movies_tags['genres'].str.replace('|', ' ') + ' ' + movies_tags['tag'].fillna('')

movies_tags.head()

"""**Insight:**
- feature ```movies_tags['combined']``` adalah feature yang berisi gabungan data dari feature ```movies_tags['genres']``` dan juga ```movies_tags['tag']```.
- Tujuan dari operasi ini adalah untuk membuat data siap diolah menggunakan TF-IDF dan penggabungan ini dapat memperkaya film karena genre memberikan informasi umum tentang film dan tag memberikan deskripsi lebih spesifik dari pengguna.

## TF-IDF Vectorizer

Proses ini digunakan untuk mengubah teks (genre + tag film) menjadi representasi numerik menggunakan teknik TF-IDF (Term Frequency-Inverse Document Frequency), sehingga bisa dihitung kemiripannya (cosine similarity) untuk sistem rekomendasi berbasis konten (Content-Based Filtering).
"""

movie_tags_copy = movies_tags.copy()

movie_tags_copy.info()

tfidf = TfidfVectorizer(stop_words='english', max_features=5000, ngram_range=(1,2))
tfidf_matrix = tfidf.fit_transform(movie_tags_copy['combined'])

tfidf_matrix.shape

"""**Insight:** Dataset ini terdiri dari 19.545 film dan dari kolom 'combined' (kombinasi dat pada features genres dan tags) TF-IDF Vectorizer menghasilkan 23.865 kata unik setelah stop words dihapus.

## Pengambilan Data Sampel Rating
"""

rating.shape

"""Dataset yang ada pada DataFrame rating terlalu banyak (20.000.263), sehingga menurut saya akan lebih baik jika menggunakan jumlah data ideal (2000.000, 10% dari data total) dari dataset tersebut agar model train dilakukan dengan lebih efisien namun tetap memberikan performa yang baik."""

ratings = rating.copy()

ratings = ratings.sample(2000_000, random_state=42)

ratings.shape

"""Jumlah dataset berhasil diperkecil dengan hanya mengambil 10% (2000.000) data dari data keseluruhan, yaitu 20.000.263

## Encode userId dan movieId

Pada tahap ini melakukan preprocessing data rating untuk mempersiapkan input yang dibutuhkan oleh model Collaborative Filtering (filter kolaboratif)
"""

user_ids = ratings['userId'].unique()
movie_ids = ratings['movieId'].unique()

user_to_idx = {user: idx for idx, user in enumerate(user_ids)}
movie_to_idx = {movie: idx for idx, movie in enumerate(movie_ids)}

x_user = np.array([user_to_idx[u] for u in ratings['userId']], dtype=np.int32)
x_movie = np.array([movie_to_idx[m] for m in ratings['movieId']], dtype=np.int32)

min_rating, max_rating = ratings['rating'].min(), ratings['rating'].max()
y = ((ratings['rating'].values - min_rating) / (max_rating - min_rating)).astype(np.float32)

x = np.column_stack((x_user, x_movie))

"""## Membagi Data untuk Training dan Validasi (Collaborative Filtering)"""

indices = np.random.permutation(len(x))
train_size = int(0.8 * len(x))
train_idx, val_idx = indices[:train_size], indices[train_size:]

x_train, y_train = x[train_idx], y[train_idx]
x_val, y_val = x[val_idx], y[val_idx]

train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train))
train_ds = train_ds.shuffle(10000).batch(128).prefetch(tf.data.AUTOTUNE)

val_ds = tf.data.Dataset.from_tensor_slices((x_val, y_val))
val_ds = val_ds.batch(128).prefetch(tf.data.AUTOTUNE)

print(f"Total data: {len(x)}")
print(f"Data training: {len(x_train)} ({len(x_train)/len(x)*100:.1f}%)")
print(f"Data validation: {len(x_val)} ({len(x_val)/len(x)*100:.1f}%)")

"""**Insight:**
- Dataset sampel sudah berhasil di split menjadi data training dan data validation dengan perbandingan 80:20.
- 80% dari 2000.000 data digunakan sebagai data training (160.000)
- 20% dari 2000.000 data digunakan sebagai data validation (40.000)

# Model Development Content Based Filtering

Content-Based Filtering merekomendasikan item berdasarkan kesamaan fitur dengan item yang disukai pengguna di masa lalu. Misalnya, jika Anda sering menonton film sci-fi seperti Interstellar, sistem akan menganalisis genre, sutradara, atau aktor dari film tersebut, lalu merekomendasikan film sci-fi lain seperti The Martian. Kelebihannya: cocok untuk cold start problem (item/user baru), tapi terbatas pada rekomendasi serupa (kurang beragam).

## Cosine Similarity
"""

cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)
cosine_sim_df = pd.DataFrame(cosine_sim, index=movie_tags_copy['title'], columns=movie_tags_copy['title'])

cosine_sim_df.head()

"""- Kode tersebut menghitung kemiripan antar film berdasarkan representasi TF-IDF dari genre dan tag, menggunakan cosine similarity.
- Diagonal (1.000000): Setiap film memiliki kemiripan sempurna dengan dirinya sendiri.

- Nilai Off-Diagonal:
  * Toy Story vs Jumanji: 0.064998 â†’ Kemiripan rendah (meski sama-sama film anak, genre/tag-nya berbeda).
  * Waiting to Exhale vs Sabrina: 0.360625 â†’ Kemiripan cukup tinggi (mungkin karena genre romance/drama).
  * Nilai 0.000000: Tidak ada kemiripan (tidak ada term yang sama di TF-IDF).

## Mendapatkan Rekomendasi
"""

def movie_recommendations(title, k=5):
    if title not in cosine_sim_df:
        return f"Movie '{title}' not found."
    sim_scores = cosine_sim_df[title].sort_values(ascending=False).iloc[1:k+1]
    return sim_scores

print("Film yang direkomendasikan berdasarkan content pilihan:")
movie_recommendations("Toy Story (1995)")

"""**Insight:**
- Rekomendasi sangat relevan karena semua film yang direkomendasikan adalah animasi dari genre yang sama (family/animasi) dan Toy Story 2 sebagai sekuel langsung mendapatkan skor tertinggi (0.936), yang sangat masuk akal.
- Skor kesamaan cukup tinggi (>0.76) untuk semua rekomendasi yang menunjukkan bahwa film-film ini memiliki banyak kesamaan dengan Toy Story. Namun ada penurunan skor yang gradual dari rekomendasi pertama ke kelima yang menunjukkan variasi dalam tingkat kesamaan.

## Evaluasi Precision@K, Recall@K, dan NDCG@K
"""

import numpy as np

def ndcg_at_k(relevance_scores, k):
    dcg = 0.0
    for i in range(k):
        rel = relevance_scores[i]
        dcg += (2**rel - 1) / np.log2(i + 2)
    ideal = sorted(relevance_scores, reverse=True)
    idcg = 0.0
    for i in range(min(k, len(ideal))):
        idcg += (2**ideal[i] - 1) / np.log2(i + 2)
    return dcg / idcg if idcg > 0 else 0.0

def evaluate_cbf_average(k=5, n_users=50):
    precision_list = []
    recall_list = []
    ndcg_list = []

    users = ratings['userId'].drop_duplicates().sample(n_users, random_state=42)

    for user_id in users:
        user_movies = ratings[ratings['userId'] == user_id]
        top_rated = user_movies[user_movies['rating'] >= 4.0]['movieId'].tolist()
        if not top_rated:
            continue

        sample_movie_id = top_rated[0]
        title_row = movies_tags[movies_tags['movieId'] == sample_movie_id]
        if title_row.empty:
            continue

        sample_title = title_row['title'].values[0]
        recs = movie_recommendations(sample_title, k=k)
        rec_titles = recs.index.tolist()

        watched_titles = movies_tags[movies_tags['movieId'].isin(user_movies['movieId'])]['title'].tolist()
        relevance = [1 if title in watched_titles else 0 for title in rec_titles]

        precision = sum(relevance) / k
        recall = sum(relevance) / len(watched_titles) if watched_titles else 0
        ndcg = ndcg_at_k(relevance, k)

        precision_list.append(precision)
        recall_list.append(recall)
        ndcg_list.append(ndcg)

    print(f"Evaluasi rata-rata untuk {len(precision_list)} user:")
    print(f"Average Precision@{k}: {np.mean(precision_list):.4f}")
    print(f"Average Recall@{k}: {np.mean(recall_list):.4f}")
    print(f"Average NDCG@{k}: {np.mean(ndcg_list):.4f}")

evaluate_cbf_average(k=10, n_users=100)

"""**Insight:**
- Average Precision@10 bernilai 0.0044 yang artinya dari 10 film yang direkomendasikan, hanya sekitar 0.044 (4.4%) yang pernah ditonton oleh user.
- Average Recall@10 bernilai 0.0061 yang artinya dari semua film yang pernah ditonton oleh user, hanya sekitar 0.61% yang berhasil direkomendasikan oleh sistem (dalam 10 film teratas).
- Average NDCG@10 bernilsi 0.0171 yang artinya urutan rekomendasi kamu hampir tidak relevan dengan minat user. Nilai NDCG ideal = 1.0 jika semua film yang direkomendasikan adalah film yang disukai user, diurutkan dengan baik.

Berdasarkan hasil di atas, dapat disimpulkan bahwa sistem Content-Based Filtering memang tidak dibangun untuk mencerminkan histori secara langsung karena tujuannya adalah untuk menemukan film baru yang mirip dengan yang disukai user sebelumnya bukan mereplikasi histori.

# Model Development dengan Collaborative Filtering

Collaborative Filtering bekerja dengan memanfaatkan perilaku pengguna lain yang mirip. Misalnya, jika Anda dan User A sama-sama menyukai Inception dan The Dark Knight, lalu User A menyukai Tenet, sistem akan merekomendasikan Tenet kepada Andaâ€”tanpa perlu tahu genre filmnya. Kelebihannya: bisa menemukan pola tak terduga, tapi rentan terhadap sparsity data dan cold start.

## Proses Training
"""

class RecommenderNet(tf.keras.Model):
    def __init__(self, num_users, num_movies, embedding_size=32):
        super().__init__()
        initializer = tf.keras.initializers.GlorotNormal()

        self.user_embedding = layers.Embedding(
            num_users, embedding_size,
            embeddings_initializer=initializer,
            embeddings_regularizer=tf.keras.regularizers.l2(1e-6))

        self.user_bias = layers.Embedding(num_users, 1)

        self.movie_embedding = layers.Embedding(
            num_movies, embedding_size,
            embeddings_initializer=initializer,
            embeddings_regularizer=tf.keras.regularizers.l2(1e-6))

        self.movie_bias = layers.Embedding(num_movies, 1)

    def call(self, inputs):
        user_vec = self.user_embedding(inputs[:, 0])
        movie_vec = self.movie_embedding(inputs[:, 1])
        user_bias = self.user_bias(inputs[:, 0])
        movie_bias = self.movie_bias(inputs[:, 1])

        dot = tf.reduce_sum(user_vec * movie_vec, axis=1, keepdims=True)
        return tf.nn.sigmoid(dot + user_bias + movie_bias)

model = RecommenderNet(len(user_ids), len(movie_ids), embedding_size=32)

optimizer = tf.keras.optimizers.Adam(
    learning_rate=0.001,
    beta_1=0.9,
    beta_2=0.999,
    epsilon=1e-07,
    clipnorm=1.0
)

model.compile(
    optimizer=optimizer,
    loss='binary_crossentropy',
    metrics=[tf.keras.metrics.RootMeanSquaredError()]
)

callbacks = [
    tf.keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True),
    tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2)
]

history = model.fit(
    train_ds,
    validation_data=val_ds,
    epochs=10,
    callbacks=callbacks,
    verbose=1
)

"""## Visualisasi Metrik"""

def plot_training_history(history):
    plt.figure(figsize=(12, 5))

    plt.subplot(1, 2, 1)
    plt.plot(history.history['loss'], label='Training Loss')
    plt.plot(history.history['val_loss'], label='Validation Loss')
    plt.title('Training and Validation Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Binary Crossentropy Loss')
    plt.legend()

    plt.subplot(1, 2, 2)
    plt.plot(history.history['root_mean_squared_error'], label='Training RMSE')
    plt.plot(history.history['val_root_mean_squared_error'], label='Validation RMSE')
    plt.title('Training and Validation RMSE')
    plt.xlabel('Epoch')
    plt.ylabel('RMSE')
    plt.legend()

    plt.tight_layout()
    plt.show()

if __name__ == "__main__":
    plot_training_history(history)

"""**Insight:**
- Epoch 0 hingga 1 mengalami penurunan loss yang tajam yang menunjukkan model belajar dengan cepat pada awal pelatihan. Epoch 2 hingga 5 penurunan loss menjadi lebih landai (melambat) yang menunjukkan model mulai mencapai stabilisasi. Dan pada epoch 6 hingga 9 training Loss terus menurun sedikit demi sedikit, validation loss mulai stagnan bahkan cenderung datar pada level sekitar 0.596.
- Validation loss tidak mengalami kenaikan yang signifikan, tidak ada overfitting, tapi juga tidak banyak perbaikan setelah epoch ke-5 dan ini menandakan bahwa model sudah mencapai titik optimal relatif terhadap kompleksitas dan jumlah data yang digunakan.
- RMSE Train menurun dari > 0.22 ke ~0.183 secara konsisten yang menunjukkan peningkatan akurasi prediksi pada data training. RMSE Val menurun dari ~0.202 ke ~0.195 dan kemudian stagnan setelah epoch ke-4.
- Penurunan RMSE menunjukkan bahwa model semakin baik dalam memprediksi nilai rating yang mendekati aktual. Gap kecil antara Training dan Validation RMSE menunjukkan bahwa model tidak mengalami overfitting dan generalisasi cukup baik terhadap data baru.

## Mendapatkan Rekomendasi
"""

def show_recommendations(user_id, n=10):
    if user_id not in user_to_idx:
        print(f"User {user_id} tidak ditemukan")
        return

    watched = ratings[ratings['userId'] == user_id]['movieId'].unique()
    unwatched = list(set(movie_ids) - set(watched))

    user_array = np.array([user_to_idx[user_id]] * len(unwatched))
    movie_array = np.array([movie_to_idx[m] for m in unwatched])
    input_array = np.column_stack((user_array, movie_array))

    ratings_pred = model.predict(input_array, batch_size=1024, verbose=0).flatten()
    ratings_pred = ratings_pred * (max_rating - min_rating) + min_rating

    recommendations = pd.DataFrame({
        'movieId': unwatched,
        'predicted_rating': ratings_pred
    }).merge(movie, on='movieId')

    print(f"\nTop {n} Rekomendasi untuk User {user_id}:")
    display(recommendations.nlargest(n, 'predicted_rating')[['title', 'genres', 'predicted_rating']])

sample_user = ratings['userId'].sample(1).iloc[0]
show_recommendations(sample_user)

"""**Insight:**
- Semua film yang direkomendasikan memiliki prediksi rating di atas 4.0 (skala maksimal 5.0) yang menunjukkan bahwa model yakin user akan sangat menyukai film-film ini.
- Rentang prediksi rating cukup ketat (4.02â€“4.15) yang artinya model kesulitan menentukan prioritas teratas karena semua rekomendasi dianggap sangat relevan.
- Drama mendominasi (8/10 film) dengan kombinasi genre seperti Crime (The Godfather, The Shawshank Redemption), Romance (Children of Paradise), atau Film-Noir (Double Indemnity). Ada juga film non-Drama seperti dokumenter (Cosmos) dan thriller (The Usual Suspects).
"""